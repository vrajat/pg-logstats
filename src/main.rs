use clap::{Parser, ValueEnum};
use indicatif::{ProgressBar, ProgressStyle};
use log::{debug, error, info, warn};
use pg_logstats::{StderrParser, QueryAnalyzer, TimingAnalyzer, JsonFormatter, TextFormatter, Result, PgLogstatsError};
use std::fs;
use std::path::{Path, PathBuf};
use std::process;
use std::time::Instant;

#[derive(Debug, Parser)]
#[clap(name="pg-logstats", version, about = "A fast PostgreSQL log analysis tool")]
struct Arguments {
    /// Log files or directory to analyze (supports glob patterns)
    #[clap(value_name = "LOG_FILES")]
    log_files: Vec<String>,

    // Phase 1 Features
    /// Directory containing PostgreSQL log files
    #[clap(long, value_name = "DIR")]
    log_dir: Option<PathBuf>,

    /// Output format for results
    #[clap(long, value_enum, default_value = "text")]
    output_format: OutputFormat,

    /// Show only summary information (quick mode)
    #[clap(long)]
    quick: bool,

    /// Limit analysis to first N lines of each file (for large files)
    #[clap(long, value_name = "N")]
    sample_size: Option<usize>,

    // Existing options (keeping the most important ones)
    /// number of minutes to build the average graphs of queries and connections. Default 5 minutes.
    #[clap(short, long)]
    average_minutes: Option<u32>,

    ///number of minutes to build the histogram graphs of queries. Default 60 minutes.
    #[clap(short = 'A', long)]
    histo_average: Option<u32>,

    /// start date/time for the data to be parsed in log (either a timestamp or a time)
    #[clap(short, long, value_name = "begin")]
    begin: Option<String>,

    /// only report on entries for the given client host
    #[clap(short = 'c', long, value_name = "dbclient")]
    dbclient: Option<String>,

    /// remove comments like /* ... */ from queries. Default is to keep them.
    #[clap(short = 'C', long, value_name = "nocomment")]
    nocomment: bool,

    /// only report on entries for the given database name
    #[clap(short, long, value_name = "dbname")]
    dbname: Option<String>,

    /// client ip addresses are replaced by their DNS name.
    /// Be warned that this can really slow down pgBadger.
    #[clap(short = 'D', long, value_name = "dns-resolve", action)]
    dns_resolve: bool,

    /// end date/time for the data to be parsed in log (either a timestamp or a time)
    #[clap(short, long, value_name = "end")]
    end: Option<String>,

    /// explode the main report by generating one report
    /// per database. Global information not related to a
    /// database are added to the postgres database report.
    #[clap(short = 'E', long, value_name = "explode")]
    explode: bool,

    /// Use this option when pgBadger is not able to detect the log format
    #[clap(short, long)]
    #[arg(value_enum)]
    format: Option<Format>,

    /// disable graphs on HTML output. Enabled by default.
    #[clap(short = 'G', long, value_name = "nograph")]
    nograph: bool,

    /// only report on entries for the given application name
    /// (PostgreSQL 9.0+ only)
    #[clap(short = 'N', long, value_name = "appname")]
    appname: Option<String>,

    /// path to directory where HTML report must be written in incremental mode, binary files stay on directory defined with -O, --outdir option.
    #[clap(short = 'H', long, value_name = "html-outdir")]
    html_outdir: Option<String>,

    /// programname used as syslog ident.
    #[clap(short = 'i', long, value_name = "ident", default_value = "postgres")]
    ident: Option<String>,

    /// use incremental mode, reports will be generated by days in a separate directory, --outdir must be set.
    #[clap(short = 'I', long, value_name = "incremental")]
    incremental: bool,

    /// number of jobs to run at same time. Run as single by default or when working with csvlog.
    #[clap(short = 'j', long, value_name = "jobs")]
    jobs: Option<u32>,

    /// number of log file to parse in parallel. Process one file at a time by default or when csvlog is used.
    #[clap(short = 'J', long, value_name = "Jobs")]
    parallel_parse: Option<u32>,

    /// allow incremental log parsing by registering the last datetime and line parsed. Useful if you want to watch errors since last run or if you want one report per day with a log rotated each week.
    #[clap(short = 'l', long, value_name = "last-parsed")]
    last_parsed: Option<String>,

    /// file containing a list of log file to parse.
    #[clap(short = 'L', long, value_name = "logfile-list")]
    logfile_list: Option<String>,

    /// maximum length of a query, it will be restricted to the given size. Default truncate size is 100000.
    #[clap(short = 'm', long, value_name = "maxlength")]
    maxlength: Option<u32>,

    /// do not collect multiline statement to avoid garbage especially on errors that generate a huge report.
    #[clap(short = 'M', long, value_name = "no-multiline")]
    no_multiline: bool,

    /// disable SQL code highlighting.
    #[clap(short = 'n', long, value_name = "nohighlight")]
    nohighlight: bool,

    /// define the filename for the output. Default depends on the output format: out.html, out.txt, out.bin, out.json or out.tsung. This option can be used multiple time to output several format. To use json output the Perl module JSON::XS must be installed, To dump output to stdout use - as filename.
    #[clap(short = 'o', long, value_name = "outfile")]
    outfile: Option<String>,

    /// directory where out file must be saved.
    #[clap(short = 'O', long, value_name = "outdir")]
    outdir: Option<String>,

    /// the value of your custom log_line_prefix as defined in your postgresql.conf. Only use it if you aren't using one of the standard prefixes specified in the pgBadger documentation, such as if your prefix includes additional variables like client ip or application name. See examples below.
    #[clap(short = 'p', long, value_name = "prefix")]
    prefix: Option<String>,

    /// disable SQL queries prettify formatter.
    #[clap(short = 'P', long, value_name = "no-prettify")]
    no_prettify: bool,

    /// don't print anything to stdout, not even a progress bar.
    #[clap(short = 'q', long, value_name = "quiet")]
    quiet: bool,

    /// add numbering of queries to the output when using options --dump-all-queries or --normalized-only.
    #[clap(short = 'Q', long, value_name = "query-numbering")]
    query_numbering: bool,

    /// set the host where to execute the cat command on remote logfile to parse locally the file.
    #[clap(short = 'r', long, value_name = "remote-host")]
    remote_host: Option<String>,

    /// number of weeks to keep in incremental mode. Default to 0, disabled. Used to set the number of weeks to keep in output directory. Older weeks and days directory are automatically removed.
    #[clap(short = 'R', long, value_name = "retention")]
    retention: Option<u32>,

    /// number of query samples to store.
    #[clap(short = 's', long, value_name = "sample", default_value = "3")]
    sample: Option<u32>,

    /// only report SELECT queries.
    #[clap(short = 'S', long, value_name = "select-only")]
    select_only: bool,

    /// change title of the HTML page report.
    #[clap(short = 'T', long, value_name = "title")]
    title: Option<String>,

    /// only report on entries for the given user.
    #[clap(short = 'u', long, value_name = "dbuser")]
    dbuser: Option<String>,

    /// exclude entries for the specified user from report. Can be used multiple time.
    #[clap(short = 'U', long, value_name = "exclude-user")]
    exclude_user: Option<String>,

    /// enable verbose or debug mode. Disabled by default.
    #[clap(short = 'v', long, value_name = "verbose", default_value = "false")]
    verbose: bool,

    /// only report errors just like logwatch could do.
    #[clap(short = 'w', long, value_name = "watch-mode")]
    watch_mode: bool,

    /// encode html output of queries into UTF8 to avoid Perl message "Wide character in print".
    #[clap(short = 'W', long, value_name = "wide-char")]
    wide_char: bool,

    /// output format. Values: text, html, bin, json or tsung.
    #[clap(short = 'x', long, value_name = "extension", default_value = "html")]
    extension: Option<String>,

    /// in incremental mode allow pgBadger to write CSS and JS files in the output directory as separate files.
    #[clap(short = 'X', long, value_name = "extra-files")]
    extra_files: bool,

    /// set the full path to the zcat program. Use it if zcat or bzcat or unzip is not in your path.
    #[clap(short = 'z', long, value_name = "zcat")]
    zcat: Option<String>,

    /// Set the number of hours from GMT of the timezone. Use this to adjust date/time in JavaScript graphs.
    #[clap(short = 'Z', long, value_name = "timezone")]
    timezone: Option<String>,

    /// pie data lower than num% will show a sum instead.
    #[clap(long, value_name = "pie-limit")]
    pie_limit: Option<u32>,

    /// any query matching the given regex will be excluded from the report. For example: "^(VACUUM|COMMIT)"
    #[clap(long, value_name = "exclude-query")]
    exclude_query: Option<String>,

    /// path of the file which contains all the regex to use to exclude queries from the report. One regex per line.
    #[clap(long, value_name = "exclude-file")]
    exclude_file: Option<String>,

    /// any query that does not match the given regex will be excluded from the report. You can use this option multiple times. For example: "(tbl1|tbl2)".
    #[clap(long, value_name = "include-query")]
    include_query: Option<String>,

    /// path of the file which contains all the regex of the queries to include from the report. One regex per line.
    #[clap(long, value_name = "include-file")]
    include_file: Option<String>,

    /// do not generate error report.
    #[clap(long, value_name = "disable-error")]
    disable_error: bool,

    /// do not generate hourly report.
    #[clap(long, value_name = "disable-hourly")]
    disable_hourly: bool,

    /// do not generate report of queries by type, database or user.
    #[clap(long, value_name = "disable-type")]
    disable_type: bool,

    /// do not generate query reports (slowest, most frequent, queries by users, by database, ...).
    #[clap(long, value_name = "disable-query")]
    disable_query: bool,

    /// do not generate session report.
    #[clap(long, value_name = "disable-session")]
    disable_session: bool,

    /// do not generate connection report.
    #[clap(long, value_name = "disable-connection")]
    disable_connection: bool,

    /// do not generate lock report.
    #[clap(long, value_name = "disable-lock")]
    disable_lock: bool,

    /// do not generate temporary report.
    #[clap(long, value_name = "disable-temporary")]
    disable_temporary: bool,

    /// do not generate checkpoint/restartpoint report.
    #[clap(long, value_name = "disable-checkpoint")]
    disable_checkpoint: bool,

    /// do not generate autovacuum report.
    #[clap(long, value_name = "disable-autovacuum")]
    disable_autovacuum: bool,

    /// used to set the HTML charset to be used.
    #[clap(long, value_name = "charset", default_value = "utf-8")]
    charset: Option<String>,

    /// used to set the CSV field separator
    #[clap(long, value_name = "csv-separator", default_value = ",")]
    csv_separator: Option<String>,

    /// any timestamp matching the given regex will be excluded from the report. Example: "2013-04-12 .*"
    #[clap(long, value_name = "exclude-time")]
    exclude_time: Option<String>,

    /// only timestamps matching the given regex will be included in the report. Example: "2013-04-12 .*"
    #[clap(long, value_name = "include-time")]
    include_time: Option<String>,

    /// exclude entries for the specified database from report. Example: "pg_dump". Can be used multiple time.
    #[clap(long, value_name = "exclude-db")]
    exclude_db: Option<String>,

    /// exclude entries for the specified application name from report.  Example: "pg_dump".  Can be used multiple time.
    #[clap(long, value_name = "exclude-appname")]
    exclude_appname: Option<String>,

    /// pgBadger will start to exclude any log entry that will match the given regex. Can be used multiple time.
    #[clap(long, value_name = "exclude-line")]
    exclude_line: Option<String>,

    /// exclude log entries for the specified client ip. Can be used multiple time.
    #[clap(long, value_name = "exclude-client")]
    exclude_client: Option<String>,

    /// obscure all literals in queries, useful to hide confidential data.
    #[clap(long, value_name = "anonymize")]
    anonymize: bool,

    /// prevent pgBadger to create reports in incremental mode.
    #[clap(long, value_name = "noreport")]
    noreport: bool,

    /// force pgBadger to associate log entries generated by both log_duration = on and log_statement = 'all'
    #[clap(long, value_name = "log-duration")]
    log_duration: bool,

    /// used to add a md5 sum under each query report.
    #[clap(long, value_name = "enable-checksum")]
    enable_checksum: bool,

    /// command to use to replace PostgreSQL logfile by a call to journalctl. Basically it might be: journalctl -u postgresql-9.5
    #[clap(long, value_name = "journalctl")]
    journalctl: Option<String>,

    /// set the path where the pid file must be stored. Default /tmp
    #[clap(long, value_name = "pid-dir", default_value = "/tmp")]
    pid_dir: Option<String>,

    /// set the name of the pid file to manage concurrent execution of pgBadger. Default: pgbadger.pid
    #[clap(long, value_name = "pid-file", default_value = "pgbadger.pid")]
    pid_file: Option<String>,

    /// used to rebuild all html reports in incremental output directories where there's binary data files.
    #[clap(long, value_name = "rebuild")]
    rebuild: bool,

    /// only show PgBouncer related menu in the header.
    #[clap(long, value_name = "pgbouncer-only")]
    pgbouncer_only: bool,

    /// in incremental mode, calendar's weeks start on a sunday. Use this option to start on a monday.
    #[clap(long, value_name = "start-monday")]
    start_monday: bool,

    /// in incremental mode, calendar's weeks start on a monday and respect the ISO 8601 week number, range 01 to 53, where week 1 is the first week that has at least 4 days in the new year.
    #[clap(long, value_name = "iso-week-number")]
    iso_week_number: bool,

    /// only dump all normalized query to out.txt
    #[clap(long, value_name = "normalized-only")]
    normalized_only: bool,

    /// do not process lines generated by auto_explain.
    #[clap(long, value_name = "noexplain")]
    noexplain: bool,

    /// command to execute to retrieve log entries on stdin. pgBadger will open a pipe to the command and parse log entries generated by the command.
    #[clap(long, value_name = "command")]
    command: Option<String>,

    /// disable changing process title to help identify pgbadger process, some system do not support it.
    #[clap(long, value_name = "no-process-info")]
    no_process_info: bool,

    /// dump all queries found in the log file replacing bind parameters are included in the queries at their respective placeholders position.
    #[clap(long, value_name = "dump-all-queries")]
    dump_all_queries: bool,

    /// do not remove comments from normalized queries. It can be useful if you want to distinguish between same normalized queries.
    #[clap(long, value_name = "keep-comments")]
    keep_comments: bool,

    /// disable progressbar.
    #[clap(long, value_name = "no-progressbar")]
    no_progressbar: bool,
}

#[derive(Debug, ValueEnum, Clone, Copy)]
enum OutputFormat {
    Text,
    Json,
}

#[derive(Debug, ValueEnum, Clone, Copy, Default)]
enum Format {
    #[default]
    Syslog,
    Syslog2,
    Stderr,
    Jsonlog,
    Cvs,
    Pgbouncer,
    Logplex,
    Rds,
    Redshift,
}

fn main() -> Result<()> {
    // Initialize logging
    env_logger::init();

    let args = Arguments::parse();
    let start_time = Instant::now();

    // Validate CLI arguments
    validate_arguments(&args)?;

    // Initialize progress bar if not in quiet mode
    let progress_bar = if !args.quiet {
        Some(create_progress_bar())
    } else {
        None
    };

    // Discover log files
    let log_files = discover_log_files(&args)?;

    if log_files.is_empty() {
        error!("No log files found to process");
        process::exit(1);
    }

    info!("Found {} log files to process", log_files.len());

    // Initialize parser based on format
    let parser = initialize_parser(&args)?;

    // Process log files with progress indication
    let mut all_entries = Vec::new();

    for (index, log_file) in log_files.iter().enumerate() {
        if let Some(pb) = &progress_bar {
            pb.set_message(format!("Processing {}", log_file.display()));
            pb.set_position(index as u64);
        }

        match process_log_file(log_file, &parser, &args) {
            Ok(mut entries) => {
                info!("Processed {} entries from {}", entries.len(), log_file.display());
                all_entries.append(&mut entries);
            }
            Err(e) => {
                warn!("Failed to process {}: {}", log_file.display(), e);
                continue;
            }
        }
    }

    if let Some(pb) = &progress_bar {
        pb.finish_with_message("File processing complete");
    }

    if all_entries.is_empty() {
        warn!("No log entries were successfully parsed");
        process::exit(1);
    }

    info!("Total entries parsed: {}", all_entries.len());

    // Run analytics on parsed data
    let analytics_result = run_analytics(&all_entries, &args)?;

    // Output results in requested format
    output_results(&analytics_result, &args, &all_entries)?;

    let elapsed = start_time.elapsed();
    if !args.quiet {
        println!("Analysis completed in {:.2}s", elapsed.as_secs_f64());
    }

    Ok(())
}

fn validate_arguments(args: &Arguments) -> Result<()> {
    // Check if log directory exists and is readable
    if let Some(log_dir) = &args.log_dir {
        if !log_dir.exists() {
            return Err(PgLogstatsError::Configuration {
                message: format!("Log directory does not exist: {}", log_dir.display()),
                field: Some("log_dir".to_string()),
            });
        }

        if !log_dir.is_dir() {
            return Err(PgLogstatsError::Configuration {
                message: format!("Log directory path is not a directory: {}", log_dir.display()),
                field: Some("log_dir".to_string()),
            });
        }

        // Test readability
        match fs::read_dir(log_dir) {
            Ok(_) => {}
            Err(e) => {
                return Err(PgLogstatsError::Configuration {
                    message: format!("Cannot read log directory {}: {}", log_dir.display(), e),
                    field: Some("log_dir".to_string()),
                });
            }
        }
    }

    // Validate sample size
    if let Some(sample_size) = args.sample_size {
        if sample_size == 0 {
            return Err(PgLogstatsError::Configuration {
                message: "Sample size must be greater than 0".to_string(),
                field: Some("sample_size".to_string()),
            });
        }
    }

    // Validate output directory if specified
    if let Some(outdir) = &args.outdir {
        let outdir_path = Path::new(outdir);
        if outdir_path.exists() && !outdir_path.is_dir() {
            return Err(PgLogstatsError::Configuration {
                message: format!("Output directory path exists but is not a directory: {}", outdir),
                field: Some("outdir".to_string()),
            });
        }
    }

    Ok(())
}

fn discover_log_files(args: &Arguments) -> Result<Vec<PathBuf>> {
    let mut log_files = Vec::new();

    // If log_dir is specified, discover files in that directory
    if let Some(log_dir) = &args.log_dir {
        discover_files_in_directory(log_dir, &mut log_files)?;
    }

    // Add explicitly specified log files
    for file_pattern in &args.log_files {
        if let Ok(path) = PathBuf::from(file_pattern).canonicalize() {
            if path.is_file() {
                log_files.push(path);
            }
        } else {
            // Try glob pattern matching (simplified implementation)
            let path = Path::new(file_pattern);
            if path.exists() && path.is_file() {
                log_files.push(path.to_path_buf());
            }
        }
    }

    // If logfile_list is specified, read file list
    if let Some(logfile_list) = &args.logfile_list {
        let list_content = fs::read_to_string(logfile_list).map_err(|e| PgLogstatsError::Io(e))?;

        for line in list_content.lines() {
            let line = line.trim();
            if !line.is_empty() && !line.starts_with('#') {
                let path = Path::new(line);
                if path.exists() && path.is_file() {
                    log_files.push(path.to_path_buf());
                }
            }
        }
    }

    // Remove duplicates and sort
    log_files.sort();
    log_files.dedup();

    // Warn about empty files
    log_files.retain(|path| {
        match fs::metadata(path) {
            Ok(metadata) => {
                if metadata.len() == 0 {
                    warn!("Skipping empty log file: {}", path.display());
                    false
                } else {
                    true
                }
            }
            Err(e) => {
                warn!("Cannot read metadata for {}: {}", path.display(), e);
                false
            }
        }
    });

    Ok(log_files)
}

fn discover_files_in_directory(dir: &Path, log_files: &mut Vec<PathBuf>) -> Result<()> {
    let entries = fs::read_dir(dir)?;

    for entry in entries {
        let entry = entry?;
        let path = entry.path();

        if path.is_file() {
            // Check if it looks like a log file
            if let Some(extension) = path.extension() {
                let ext_str = extension.to_string_lossy().to_lowercase();
                if ext_str == "log" || ext_str == "txt" {
                    log_files.push(path);
                }
            } else if let Some(filename) = path.file_name() {
                let filename_str = filename.to_string_lossy().to_lowercase();
                if filename_str.contains("postgres") || filename_str.contains("pg") {
                    log_files.push(path);
                }
            }
        }
    }

    Ok(())
}

fn initialize_parser(_args: &Arguments) -> Result<StderrParser> {
    // For now, we'll use StderrParser as the default
    // In the future, we can add logic to choose parser based on format
    debug!("Initializing stderr parser");
    Ok(StderrParser::new())
}

fn process_log_file(
    log_file: &Path,
    parser: &StderrParser,
    args: &Arguments,
) -> Result<Vec<pg_logstats::LogEntry>> {
    let content = fs::read_to_string(log_file)?;
    let lines: Vec<String> = content.lines().map(|s| s.to_string()).collect();

    // Apply sample size limit if specified
    let lines_to_process = if let Some(sample_size) = args.sample_size {
        if lines.len() > sample_size {
            info!("Limiting analysis to first {} lines of {}", sample_size, log_file.display());
            &lines[..sample_size]
        } else {
            &lines
        }
    } else {
        &lines
    };

    parser.parse_lines(lines_to_process)
}

fn run_analytics(
    entries: &[pg_logstats::LogEntry],
    _args: &Arguments,
) -> Result<pg_logstats::AnalysisResult> {
    info!("Running analytics on {} entries", entries.len());

    let query_analyzer = QueryAnalyzer::new();
    let timing_analyzer = TimingAnalyzer::new();

    // Run query analysis
    let analysis_result = query_analyzer.analyze(entries)?;

    // Run timing analysis
    let _timing_analysis = timing_analyzer.analyze_timing(entries)?;

    Ok(analysis_result)
}

fn output_results(
    analytics_result: &pg_logstats::AnalysisResult,
    args: &Arguments,
    entries: &[pg_logstats::LogEntry],
) -> Result<()> {
    match args.output_format {
        OutputFormat::Json => {
            let formatter = JsonFormatter::new()
                .with_pretty(true)
                .with_metadata(env!("CARGO_PKG_VERSION"), vec![], entries.len());

            let output = formatter.format(analytics_result)?;

            if let Some(outfile) = &args.outfile {
                if outfile == "-" {
                    println!("{}", output);
                } else {
                    fs::write(outfile, output)?;
                    info!("Results written to {}", outfile);
                }
            } else {
                println!("{}", output);
            }
        }
        OutputFormat::Text => {
            let formatter = TextFormatter::new();
            let output = formatter.format_query_analysis(analytics_result)?;

            if let Some(outfile) = &args.outfile {
                if outfile == "-" {
                    println!("{}", output);
                } else {
                    fs::write(outfile, output)?;
                    info!("Results written to {}", outfile);
                }
            } else {
                println!("{}", output);
            }
        }
    }

    Ok(())
}

fn create_progress_bar() -> ProgressBar {
    let pb = ProgressBar::new(100);
    pb.set_style(
        ProgressStyle::default_bar()
            .template("{spinner:.green} [{elapsed_precise}] [{bar:40.cyan/blue}] {pos}/{len} {msg}")
            .unwrap()
            .progress_chars("#>-"),
    );
    pb
}
